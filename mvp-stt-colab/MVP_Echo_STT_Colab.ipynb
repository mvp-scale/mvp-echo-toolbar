{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU",
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0c7d08a764874c168699b8a59ba2ad9f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_c360df2e95c844d6aa99cb25a06a48e1",
       "IPY_MODEL_d7f8001ec6dc44888e3b622f2fe6f8bc",
       "IPY_MODEL_af27605235694b4883a782e3de1fd69e"
      ],
      "layout": "IPY_MODEL_1eb5711173864e47ac301ca52c84dbe4"
     }
    },
    "c360df2e95c844d6aa99cb25a06a48e1": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_169db064ea134416b676865c47e7213f",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_8eb7dfd55919488c840db6732c10a11e",
      "value": "Downloadâ€‡complete:â€‡"
     }
    },
    "d7f8001ec6dc44888e3b622f2fe6f8bc": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "info",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_992ffe1130a740ebbd81c8b7d7464c33",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_08fecca9b1d0460ba4e8845cf19d135c",
      "value": 1
     }
    },
    "af27605235694b4883a782e3de1fd69e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_96e79373b0d24327931372ceb64b2d65",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_9c777e10c4934368adc1451f8cb0bf41",
      "value": "â€‡661M/?â€‡[00:20&lt;00:00,â€‡70.1MB/s]"
     }
    },
    "1eb5711173864e47ac301ca52c84dbe4": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "169db064ea134416b676865c47e7213f": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8eb7dfd55919488c840db6732c10a11e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "992ffe1130a740ebbd81c8b7d7464c33": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "08fecca9b1d0460ba4e8845cf19d135c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "96e79373b0d24327931372ceb64b2d65": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9c777e10c4934368adc1451f8cb0bf41": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "caef863c380c4d65a554ed597d0f4590": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_ef5d55e5a93e4444b69aee9050729674",
       "IPY_MODEL_39e324b68e0f43c5b98c244021382628",
       "IPY_MODEL_2fdfa6e7cddb4e51b1871dd22981f3d1"
      ],
      "layout": "IPY_MODEL_42bb496b13c04439ad62f10971f28f78"
     }
    },
    "ef5d55e5a93e4444b69aee9050729674": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_807a1dc139c34ba983925c93068c4663",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_b5ccb3b5d4444a3f8cf44ed2f7db649b",
      "value": "Fetchingâ€‡7â€‡files:â€‡100%"
     }
    },
    "39e324b68e0f43c5b98c244021382628": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_56d2a39a29b747199c0a70fc6a0a7171",
      "max": 7,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_9b45668eb0c948a3b6fa134b81c1167b",
      "value": 7
     }
    },
    "2fdfa6e7cddb4e51b1871dd22981f3d1": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_679e713dc3834fb49ef9b69e93079ad7",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_6f307a29e1ac49668c4f771dc9d98105",
      "value": "â€‡7/7â€‡[00:06&lt;00:00,â€‡â€‡1.90s/it]"
     }
    },
    "42bb496b13c04439ad62f10971f28f78": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "807a1dc139c34ba983925c93068c4663": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b5ccb3b5d4444a3f8cf44ed2f7db649b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "56d2a39a29b747199c0a70fc6a0a7171": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9b45668eb0c948a3b6fa134b81c1167b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "679e713dc3834fb49ef9b69e93079ad7": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6f307a29e1ac49668c4f771dc9d98105": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rMqSMbSsJ8w_"
   },
   "source": "# MVP-Echo STT â€” Try It on Colab\n\nRun **local speech-to-text** powered by [NVIDIA Parakeet](https://huggingface.co/csukuangfj/sherpa-onnx-nemo-parakeet-tdt-0.6b-v2-int8) on a free Colab GPU.\n\n**No local GPU required.** This notebook gives you a temporary OpenAI-compatible\n`/v1/audio/transcriptions` endpoint backed by a T4 GPU.\n\n| | |\n|---|---|\n| **Model** | Parakeet TDT 0.6b v2 (INT8) â€” English |\n| **Engine** | [sherpa-onnx](https://github.com/k2-fsa/sherpa-onnx) C++ with CUDA |\n| **Speed** | ~20-50x faster than real-time |\n| **VRAM** | ~426 MiB (fits easily on T4's 16GB) |\n| **API** | OpenAI Whisper-compatible |\n| **API Key** | `SK-COLAB-COMMUNITY` |\n\n**How to use:**\n1. Make sure the runtime is set to **GPU** (Runtime â†’ Change runtime type â†’ T4 GPU)\n2. Run all cells in order (Runtime â†’ Run all)\n3. Copy the public URL printed at the end\n4. In MVP-Echo toolbar settings, set the endpoint URL and use API key: **`SK-COLAB-COMMUNITY`**\n\n> **Connecting from MVP-Echo toolbar:** Enter the Colab public URL as the server\n> endpoint and use `SK-COLAB-COMMUNITY` as the API key. The toolbar requires a key\n> to complete the connection test â€” this well-known community key fulfills that check.\n> Any non-empty key will work.\n\n> This is a **community resource** for developers who don't have a local GPU.\n> The session is temporary and will shut down when Colab reclaims the runtime.\n> No data is stored. No telemetry. Audio is processed and immediately deleted."
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bLhok6-lJ8xA"
   },
   "source": [
    "---\n",
    "## 1. Verify GPU"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RMLyA9Q0J8xA",
    "outputId": "02c66f93-9347-4d91-e5a8-a22aaa9722d4"
   },
   "source": [
    "!nvidia-smi\n",
    "\n",
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    vram = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n",
    "    print(f\"\\nGPU: {gpu_name} ({vram:.1f} GB VRAM)\")\n",
    "    print(\"CUDA is available â€” good to go.\")\n",
    "else:\n",
    "    print(\"\\nNo GPU detected!\")\n",
    "    print(\"Go to Runtime â†’ Change runtime type â†’ select T4 GPU\")"
   ],
   "execution_count": 6,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Sun Feb 15 21:17:25 2026       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 580.82.07              Driver Version: 580.82.07      CUDA Version: 13.0     |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
      "| N/A   41C    P8             13W /   70W |       3MiB /  15360MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "\n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n",
      "\n",
      "GPU: Tesla T4 (14.6 GB VRAM)\n",
      "CUDA is available â€” good to go.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BI9GjTZlJ8xA"
   },
   "source": [
    "---\n",
    "## 2. Install sherpa-onnx + dependencies\n",
    "\n",
    "Downloads pre-built C++ binaries with CUDA 12 + cuDNN 9 support (~234 MB)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RUCBr2nJJ8xA",
    "outputId": "0990b8d9-316f-4eab-99f5-bdeb1635519b"
   },
   "source": [
    "import os\n",
    "\n",
    "SHERPA_VERSION = \"v1.12.23\"\n",
    "SHERPA_URL = (\n",
    "    f\"https://github.com/k2-fsa/sherpa-onnx/releases/download/{SHERPA_VERSION}/\"\n",
    "    f\"sherpa-onnx-{SHERPA_VERSION}-cuda-12.x-cudnn-9.x-linux-x64-gpu.tar.bz2\"\n",
    ")\n",
    "\n",
    "if not os.path.exists(\"/opt/sherpa-onnx/bin/sherpa-onnx-offline-websocket-server\"):\n",
    "    print(\"Downloading sherpa-onnx pre-built binaries...\")\n",
    "    !curl -fSL -o /tmp/sherpa.tar.bz2 \"{SHERPA_URL}\"\n",
    "    !mkdir -p /opt/sherpa-onnx\n",
    "    !tar xjf /tmp/sherpa.tar.bz2 -C /opt/sherpa-onnx --strip-components=1\n",
    "    !rm /tmp/sherpa.tar.bz2\n",
    "    print(\"sherpa-onnx installed.\")\n",
    "else:\n",
    "    print(\"sherpa-onnx already installed.\")\n",
    "\n",
    "# Set PATH and LD_LIBRARY_PATH for this session\n",
    "os.environ[\"PATH\"] = f\"/opt/sherpa-onnx/bin:{os.environ['PATH']}\"\n",
    "os.environ[\"LD_LIBRARY_PATH\"] = f\"/opt/sherpa-onnx/lib:{os.environ.get('LD_LIBRARY_PATH', '')}\"\n",
    "\n",
    "# Verify\n",
    "!sherpa-onnx-offline-websocket-server --help 2>&1 | head -3\n",
    "print(\"\\nsherpa-onnx is ready.\")"
   ],
   "execution_count": 7,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "sherpa-onnx already installed.\n",
      "/home/runner/work/sherpa-onnx/sherpa-onnx/sherpa-onnx/csrc/parse-options.cc:PrintUsage:414 \n",
      "\n",
      "Automatic speech recognition with sherpa-onnx using websocket.\n",
      "\n",
      "sherpa-onnx is ready.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "tsnsvW1xJ8xA"
   },
   "source": [
    "# Install Python dependencies\n",
    "!pip install -q fastapi \"uvicorn[standard]\" python-multipart soundfile numpy websockets"
   ],
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SStiRhqnJ8xA"
   },
   "source": "---\n## 3. Download model\n\nDownloads the Parakeet TDT 0.6b v2 (INT8) model from HuggingFace (~300 MB).\nThis is NVIDIA's English speech recognition model, quantized for fast inference.\n\n### HuggingFace Token (optional but recommended)\n\nThe model is on a **public repo**, so it will download without a token. However,\nHuggingFace may rate-limit anonymous downloads. Adding a token avoids this.\n\n**How to get a HuggingFace token:**\n1. Create a free account at [huggingface.co](https://huggingface.co/join)\n2. Go to [Settings â†’ Access Tokens](https://huggingface.co/settings/tokens)\n3. Click **\"New token\"** â†’ name it anything (e.g. \"Colab\") â†’ select **\"Read\"** access\n4. Copy the token (starts with `hf_...`)\n\n**Two ways to add it in Colab:**\n\n**Option 1 â€” Colab Secrets (recommended, more secure):**\n1. Click the **ðŸ”‘ key icon** in the left sidebar\n2. Click **\"Add new secret\"**\n3. Name: `HF_TOKEN` â†’ Value: paste your token\n4. Toggle **\"Notebook access\"** ON\n\n**Option 2 â€” Paste directly in the cell below** (less secure, visible in notebook)"
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 220,
     "referenced_widgets": [
      "0c7d08a764874c168699b8a59ba2ad9f",
      "c360df2e95c844d6aa99cb25a06a48e1",
      "d7f8001ec6dc44888e3b622f2fe6f8bc",
      "af27605235694b4883a782e3de1fd69e",
      "1eb5711173864e47ac301ca52c84dbe4",
      "169db064ea134416b676865c47e7213f",
      "8eb7dfd55919488c840db6732c10a11e",
      "992ffe1130a740ebbd81c8b7d7464c33",
      "08fecca9b1d0460ba4e8845cf19d135c",
      "96e79373b0d24327931372ceb64b2d65",
      "9c777e10c4934368adc1451f8cb0bf41",
      "caef863c380c4d65a554ed597d0f4590",
      "ef5d55e5a93e4444b69aee9050729674",
      "39e324b68e0f43c5b98c244021382628",
      "2fdfa6e7cddb4e51b1871dd22981f3d1",
      "42bb496b13c04439ad62f10971f28f78",
      "807a1dc139c34ba983925c93068c4663",
      "b5ccb3b5d4444a3f8cf44ed2f7db649b",
      "56d2a39a29b747199c0a70fc6a0a7171",
      "9b45668eb0c948a3b6fa134b81c1167b",
      "679e713dc3834fb49ef9b69e93079ad7",
      "6f307a29e1ac49668c4f771dc9d98105"
     ]
    },
    "id": "WfN7wb_oJ8xA",
    "outputId": "ad117050-7f48-4dc2-cb43-8946cfb10cac"
   },
   "source": "#@title Download model { display-mode: \"form\" }\n\n#@markdown **HuggingFace token** (optional â€” leave blank to download without auth):\nHF_TOKEN = \"\"  #@param {type:\"string\"}\n\n# Try Colab Secrets first, then fall back to the field above\nhf_token = None\ntry:\n    from google.colab import userdata\n    hf_token = userdata.get(\"HF_TOKEN\")\n    if hf_token:\n        print(\"Using HuggingFace token from Colab Secrets.\")\nexcept (ImportError, userdata.SecretNotFoundError, Exception):\n    pass\n\nif not hf_token and HF_TOKEN.strip():\n    hf_token = HF_TOKEN.strip()\n    print(\"Using HuggingFace token from cell input.\")\n\nif not hf_token:\n    print(\"No HuggingFace token provided â€” downloading anonymously.\")\n    print(\"(This works fine, but may be rate-limited on heavy usage days.)\\n\")\n\nMODEL_ID = \"parakeet-tdt-0.6b-v2-int8\"\nHF_REPO = f\"csukuangfj/sherpa-onnx-nemo-{MODEL_ID}\"\nMODEL_DIR = f\"/content/models/sherpa-onnx-nemo-{MODEL_ID}\"\n\nif os.path.exists(f\"{MODEL_DIR}/tokens.txt\"):\n    print(f\"Model already downloaded: {MODEL_ID}\")\nelse:\n    print(f\"Downloading {MODEL_ID} from HuggingFace...\")\n    !pip install -q huggingface-hub\n    from huggingface_hub import snapshot_download\n    snapshot_download(\n        repo_id=HF_REPO,\n        local_dir=MODEL_DIR,\n        token=hf_token,\n    )\n    print(f\"Model downloaded to {MODEL_DIR}\")\n\n# Verify model files\nrequired = [\"encoder.int8.onnx\", \"decoder.int8.onnx\", \"joiner.int8.onnx\", \"tokens.txt\"]\nfor f in required:\n    path = os.path.join(MODEL_DIR, f)\n    size_mb = os.path.getsize(path) / (1024 * 1024) if os.path.exists(path) else 0\n    status = f\"{size_mb:.1f} MB\" if os.path.exists(path) else \"MISSING\"\n    print(f\"  {f}: {status}\")\nprint(f\"\\nModel ready: {MODEL_ID}\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bIgHcthrJ8xB"
   },
   "source": [
    "---\n",
    "## 4. Start the STT server\n",
    "\n",
    "This starts:\n",
    "1. The sherpa-onnx C++ WebSocket server (loads model into GPU memory)\n",
    "2. A FastAPI HTTP server with the OpenAI-compatible `/v1/audio/transcriptions` endpoint"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VBLWXXF2J8xB",
    "outputId": "b3929e91-acfb-428d-97b8-fa00a0042c11"
   },
   "source": "# Write the server script to disk\n# (If you cloned the repo, you can skip this â€” the file is in mvp-stt-colab/server.py)\n\nSERVER_SCRIPT = r'''\nimport asyncio, json, os, signal, struct, subprocess, tempfile, time\nimport numpy as np\nimport soundfile as sf\nimport uvicorn\nimport websockets\nfrom contextlib import asynccontextmanager\nfrom fastapi import FastAPI, File, Form, Request, UploadFile\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom fastapi.responses import JSONResponse\n\nMODEL_DIR = os.environ.get(\"MODEL_DIR\")\nPROVIDER = os.environ.get(\"PROVIDER\", \"cuda\")\nNUM_THREADS = os.environ.get(\"NUM_THREADS\", \"4\")\nPORT = int(os.environ.get(\"PORT\", \"8000\"))\nWS_PORT = int(os.environ.get(\"WS_PORT\", \"7100\"))\n\n# Community API key â€” the toolbar requires a key to connect.\n# Any non-empty Bearer token is accepted.\nCOMMUNITY_API_KEY = \"SK-COLAB-COMMUNITY\"\n\n_process = None\n\nasync def start_sherpa():\n    global _process\n    cmd = [\n        \"sherpa-onnx-offline-websocket-server\",\n        f\"--port={WS_PORT}\", f\"--provider={PROVIDER}\",\n        f\"--encoder={MODEL_DIR}/encoder.int8.onnx\",\n        f\"--decoder={MODEL_DIR}/decoder.int8.onnx\",\n        f\"--joiner={MODEL_DIR}/joiner.int8.onnx\",\n        f\"--tokens={MODEL_DIR}/tokens.txt\",\n        f\"--num-threads={NUM_THREADS}\", \"--max-utterance-length=600\",\n    ]\n    print(f\"[server] Starting sherpa-onnx...\")\n    _process = await asyncio.create_subprocess_exec(\n        *cmd, stdout=asyncio.subprocess.PIPE, stderr=asyncio.subprocess.PIPE)\n    for _ in range(60):\n        if _process.returncode is not None:\n            stderr = \"\"\n            if _process.stderr:\n                try:\n                    raw = await asyncio.wait_for(_process.stderr.read(4096), 1.0)\n                    stderr = raw.decode(errors=\"replace\")\n                except asyncio.TimeoutError: pass\n            raise RuntimeError(f\"sherpa-onnx exited: {stderr}\")\n        try:\n            async with websockets.connect(f\"ws://localhost:{WS_PORT}\",\n                                          close_timeout=2, open_timeout=2) as ws:\n                await ws.send(\"Done\")\n            print(f\"[server] sherpa-onnx ready (PID={_process.pid}, provider={PROVIDER})\")\n            return\n        except: await asyncio.sleep(0.5)\n    raise RuntimeError(\"sherpa-onnx failed to start\")\n\ndef convert_to_wav(inp, out):\n    try:\n        r = subprocess.run([\"ffmpeg\",\"-y\",\"-loglevel\",\"error\",\"-i\",inp,\n            \"-ar\",\"16000\",\"-ac\",\"1\",\"-sample_fmt\",\"s16\",\"-f\",\"wav\",out],\n            capture_output=True, timeout=30)\n        return r.returncode == 0\n    except: return False\n\nasync def transcribe_ws(samples, sample_rate):\n    async with websockets.connect(f\"ws://localhost:{WS_PORT}\") as ws:\n        header = struct.pack(\"<ii\", sample_rate, samples.size * 4)\n        buf = header + samples.tobytes()\n        for start in range(0, len(buf), 10240):\n            await ws.send(buf[start:start+10240])\n        result = await ws.recv()\n        await ws.send(\"Done\")\n    try: return json.loads(result).get(\"text\", \"\").strip()\n    except: return result.strip() if isinstance(result, str) else result.decode().strip()\n\ndef _check_auth(request):\n    auth = request.headers.get(\"authorization\", \"\")\n    if not auth.startswith(\"Bearer \"): return False\n    return len(auth[7:].strip()) > 0\n\n@asynccontextmanager\nasync def lifespan(application):\n    await start_sherpa()\n    print(f\"[server] Ready on port {PORT}\")\n    print(f\"[server] Community API key: {COMMUNITY_API_KEY}\")\n    yield\n    if _process and _process.returncode is None:\n        _process.terminate()\n\napp = FastAPI(title=\"MVP-Echo STT\", lifespan=lifespan)\napp.add_middleware(CORSMiddleware, allow_origins=[\"*\"], allow_methods=[\"*\"], allow_headers=[\"*\"])\n\n@app.get(\"/health\")\nasync def health():\n    return {\"status\": \"ok\", \"model\": os.path.basename(MODEL_DIR), \"provider\": PROVIDER}\n\n@app.get(\"/v1/models\")\nasync def list_models(request: Request):\n    if not _check_auth(request):\n        return JSONResponse(status_code=401,\n            content={\"error\": \"Invalid or missing API key. Use: SK-COLAB-COMMUNITY\"})\n    model_id = os.path.basename(MODEL_DIR)\n    for prefix in (\"sherpa-onnx-nemo-\", \"sherpa-onnx-\"):\n        if model_id.startswith(prefix):\n            model_id = model_id[len(prefix):]; break\n    return JSONResponse({\"data\": [{\"id\": model_id, \"object\": \"model\",\n        \"owned_by\": \"colab\", \"label\": \"English\", \"group\": \"gpu\", \"active\": True}]})\n\n@app.post(\"/v1/audio/transcriptions\")\nasync def transcribe(\n    file: UploadFile = File(...), model: str = Form(default=\"\"),\n    language: str = Form(default=\"en\"), response_format: str = Form(default=\"verbose_json\"),\n    temperature: str = Form(default=\"0\"),\n):\n    t0 = time.time()\n    suffix = os.path.splitext(file.filename or \"audio.webm\")[1] or \".webm\"\n    with tempfile.NamedTemporaryFile(suffix=suffix, delete=False) as tmp:\n        tmp.write(await file.read()); tmp_path = tmp.name\n    wav_path = tmp_path + \".wav\"\n    try:\n        if not convert_to_wav(tmp_path, wav_path):\n            return JSONResponse(status_code=400, content={\"error\": \"Audio conversion failed\"})\n        samples, sr = sf.read(wav_path, dtype=\"float32\")\n        if len(samples.shape) > 1: samples = samples[:, 0]\n        dur = len(samples) / sr\n        text = await transcribe_ws(samples.astype(np.float32), sr)\n        elapsed = time.time() - t0\n        print(f\"[server] {dur:.1f}s -> {elapsed:.2f}s (RTF={elapsed/dur:.3f}): \\\"{text[:80]}\\\"\")\n        if response_format == \"verbose_json\":\n            return JSONResponse({\"text\": text, \"language\": language,\n                \"duration\": round(dur, 2),\n                \"segments\": [{\"id\":0,\"start\":0.0,\"end\":round(dur,2),\"text\":text}]})\n        return JSONResponse({\"text\": text})\n    except Exception as e:\n        return JSONResponse(status_code=500, content={\"error\": str(e)})\n    finally:\n        for p in [tmp_path, wav_path]:\n            try: os.unlink(p)\n            except: pass\n\nif __name__ == \"__main__\":\n    uvicorn.run(app, host=\"0.0.0.0\", port=PORT)\n'''.strip()\n\nwith open('/content/server.py', 'w') as f:\n    f.write(SERVER_SCRIPT)\nprint('Server script written to /content/server.py')",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uDxCeETEJ8xB",
    "outputId": "e1c8d96a-b5df-428d-ec00-743d6948772a"
   },
   "source": [
    "import subprocess as _sp\n",
    "import time as _time\n",
    "import os as _os\n",
    "\n",
    "# Set environment for the server process\n",
    "server_env = _os.environ.copy()\n",
    "server_env[\"MODEL_DIR\"] = MODEL_DIR\n",
    "server_env[\"PROVIDER\"] = \"cuda\"\n",
    "server_env[\"PORT\"] = \"8000\"\n",
    "\n",
    "# Start the server in the background\n",
    "server_proc = _sp.Popen(\n",
    "    [\"python3\", \"/content/server.py\"],\n",
    "    env=server_env,\n",
    "    stdout=_sp.PIPE,\n",
    "    stderr=_sp.STDOUT,\n",
    ")\n",
    "print(f\"Server starting (PID={server_proc.pid})...\")\n",
    "\n",
    "# Wait for the server to be ready\n",
    "import urllib.request\n",
    "for i in range(120):\n",
    "    try:\n",
    "        resp = urllib.request.urlopen(\"http://localhost:8000/health\", timeout=2)\n",
    "        data = resp.read().decode()\n",
    "        print(f\"\\nServer is up! Health: {data}\")\n",
    "        break\n",
    "    except Exception:\n",
    "        if server_proc.poll() is not None:\n",
    "            output = server_proc.stdout.read().decode()\n",
    "            print(f\"Server died! Output:\\n{output}\")\n",
    "            break\n",
    "        print(\".\", end=\"\", flush=True)\n",
    "        _time.sleep(1)\n",
    "else:\n",
    "    print(\"\\nServer failed to start within 120 seconds.\")\n",
    "    output = server_proc.stdout.read(4096).decode() if server_proc.stdout else \"\"\n",
    "    print(f\"Output: {output}\")"
   ],
   "execution_count": 11,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Server starting (PID=2267)...\n",
      ".............\n",
      "Server is up! Health: {\"status\":\"ok\",\"model\":\"sherpa-onnx-nemo-parakeet-tdt-0.6b-v2-int8\",\"provider\":\"cuda\"}\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kdm07W-hJ8xB"
   },
   "source": [
    "---\n",
    "## 5. Create public URL\n",
    "\n",
    "Choose **one** of the options below to expose the server with a public URL.\n",
    "\n",
    "**Option A** (cloudflared) â€” no signup needed, works immediately.\n",
    "\n",
    "**Option B** (ngrok) â€” requires a free [ngrok.com](https://ngrok.com) account."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 156
    },
    "id": "vlcH_K2NJ8xB",
    "outputId": "179f3302-3195-4e08-fb7c-0a8cb9bd3068"
   },
   "source": [
    "#@title Option A: cloudflared tunnel (no signup required)\n",
    "\n",
    "import subprocess, time, re\n",
    "\n",
    "# Install cloudflared\n",
    "!wget -q https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64 -O /usr/local/bin/cloudflared\n",
    "!chmod +x /usr/local/bin/cloudflared\n",
    "\n",
    "# Start tunnel\n",
    "tunnel_proc = subprocess.Popen(\n",
    "    [\"cloudflared\", \"tunnel\", \"--url\", \"http://localhost:8000\", \"--no-autoupdate\"],\n",
    "    stdout=subprocess.PIPE,\n",
    "    stderr=subprocess.PIPE,\n",
    ")\n",
    "\n",
    "# Wait for the URL to appear in stderr\n",
    "public_url = None\n",
    "deadline = time.time() + 30\n",
    "buffer = b\"\"\n",
    "\n",
    "import select\n",
    "while time.time() < deadline and public_url is None:\n",
    "    ready, _, _ = select.select([tunnel_proc.stderr], [], [], 1.0)\n",
    "    if ready:\n",
    "        chunk = tunnel_proc.stderr.read1(4096) if hasattr(tunnel_proc.stderr, 'read1') else tunnel_proc.stderr.read(4096)\n",
    "        if chunk:\n",
    "            buffer += chunk\n",
    "            text = buffer.decode(errors=\"replace\")\n",
    "            match = re.search(r'(https://[a-z0-9-]+\\.trycloudflare\\.com)', text)\n",
    "            if match:\n",
    "                public_url = match.group(1)\n",
    "\n",
    "if public_url:\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"PUBLIC URL: {public_url}\")\n",
    "    print(\"=\" * 60)\n",
    "    print()\n",
    "    print(f\"Health check:  {public_url}/health\")\n",
    "    print(f\"Transcribe:    POST {public_url}/v1/audio/transcriptions\")\n",
    "    print()\n",
    "    print(\"This URL is temporary and will stop when the Colab session ends.\")\n",
    "else:\n",
    "    print(\"Failed to get tunnel URL. Check the output above for errors.\")\n",
    "    print(\"Stderr:\", buffer.decode(errors=\"replace\")[:500])"
   ],
   "execution_count": 12,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "============================================================\n",
      "PUBLIC URL: https://tent-enjoy-films-driver.trycloudflare.com\n",
      "============================================================\n",
      "\n",
      "Health check:  https://tent-enjoy-films-driver.trycloudflare.com/health\n",
      "Transcribe:    POST https://tent-enjoy-films-driver.trycloudflare.com/v1/audio/transcriptions\n",
      "\n",
      "This URL is temporary and will stop when the Colab session ends.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D8apYrAkJ8xC",
    "outputId": "d4770c80-f8f7-47b5-e6d2-887404f67a36"
   },
   "source": [
    "#@title Option B: ngrok tunnel (requires free account)\n",
    "\n",
    "# 1. Sign up at https://ngrok.com (free)\n",
    "# 2. Get your auth token from https://dashboard.ngrok.com/get-started/your-authtoken\n",
    "# 3. Paste it below:\n",
    "\n",
    "NGROK_AUTH_TOKEN = \"\"  #@param {type:\"string\"}\n",
    "\n",
    "if not NGROK_AUTH_TOKEN:\n",
    "    print(\"Paste your ngrok auth token above and re-run this cell.\")\n",
    "    print(\"Get one free at: https://dashboard.ngrok.com/get-started/your-authtoken\")\n",
    "else:\n",
    "    !pip install -q pyngrok\n",
    "    from pyngrok import ngrok\n",
    "\n",
    "    ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n",
    "    tunnel = ngrok.connect(8000, \"http\")\n",
    "    public_url = tunnel.public_url\n",
    "\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"PUBLIC URL: {public_url}\")\n",
    "    print(\"=\" * 60)\n",
    "    print()\n",
    "    print(f\"Health check:  {public_url}/health\")\n",
    "    print(f\"Transcribe:    POST {public_url}/v1/audio/transcriptions\")\n",
    "    print()\n",
    "    print(\"This URL is temporary and will stop when the Colab session ends.\")"
   ],
   "execution_count": 13,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Paste your ngrok auth token above and re-run this cell.\n",
      "Get one free at: https://dashboard.ngrok.com/get-started/your-authtoken\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ygwxSIx-J8xC"
   },
   "source": [
    "---\n",
    "## 6. Test it\n",
    "\n",
    "### From this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DtZpWR12J8xC",
    "outputId": "ccf75334-1b69-4973-fcef-166a686febb8"
   },
   "source": [
    "# Quick health check\n",
    "!curl -s http://localhost:8000/health | python3 -m json.tool"
   ],
   "execution_count": 14,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{\n",
      "    \"status\": \"ok\",\n",
      "    \"model\": \"sherpa-onnx-nemo-parakeet-tdt-0.6b-v2-int8\",\n",
      "    \"provider\": \"cuda\"\n",
      "}\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u7fsfdJaJ8xC",
    "outputId": "4d37790e-2cf9-4a04-e40a-713c00351dc6"
   },
   "source": [
    "# Record a short test clip and transcribe it\n",
    "# (This creates a 3-second sine wave as a test â€” replace with real audio for real results)\n",
    "\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "\n",
    "# Generate a silent test file (just to verify the pipeline works)\n",
    "sr = 16000\n",
    "duration = 2.0\n",
    "samples = np.zeros(int(sr * duration), dtype=np.float32)\n",
    "sf.write(\"/tmp/test_silence.wav\", samples, sr)\n",
    "\n",
    "print(\"Transcribing test audio (2s silence)...\")\n",
    "!curl -s -X POST http://localhost:8000/v1/audio/transcriptions \\\n",
    "  -F \"file=@/tmp/test_silence.wav\" \\\n",
    "  -F \"response_format=verbose_json\" | python3 -m json.tool\n",
    "\n",
    "print(\"\\nTo test with real audio, upload a file and run:\")\n",
    "print('  !curl -X POST http://localhost:8000/v1/audio/transcriptions -F \"file=@/path/to/audio.wav\"')"
   ],
   "execution_count": 15,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Transcribing test audio (2s silence)...\n",
      "{\n",
      "    \"text\": \"\",\n",
      "    \"language\": \"en\",\n",
      "    \"duration\": 2.0,\n",
      "    \"segments\": [\n",
      "        {\n",
      "            \"id\": 0,\n",
      "            \"start\": 0.0,\n",
      "            \"end\": 2.0,\n",
      "            \"text\": \"\"\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "\n",
      "To test with real audio, upload a file and run:\n",
      "  !curl -X POST http://localhost:8000/v1/audio/transcriptions -F \"file=@/path/to/audio.wav\"\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ym56PGw3J8xC"
   },
   "source": "### From your machine (using the public URL):\n\n```bash\n# Health check\ncurl https://YOUR-URL.trycloudflare.com/health\n\n# Transcribe an audio file\ncurl -X POST https://YOUR-URL.trycloudflare.com/v1/audio/transcriptions \\\n  -F \"file=@recording.wav\" \\\n  -F \"response_format=verbose_json\"\n```\n\n### From MVP-Echo toolbar:\n\nSet the STT endpoint in settings to your public URL:\n```\nhttps://YOUR-URL.trycloudflare.com\n```\n\n> **About API keys:** This Colab server has **no authentication** â€” it's open access\n> for community testing. You do not need an API key. If the toolbar has an API key\n> field, you can leave it blank or enter any value â€” the server ignores it.\n> The \"Test Connection\" button may report a warning about auth, but actual\n> transcription will work fine regardless."
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-rqLwapBJ8xC",
    "outputId": "6ca470b3-95e3-4b7a-c221-eb7be7c15171"
   },
   "source": "### From your machine (using the public URL):\n\n```bash\n# Health check\ncurl https://YOUR-URL.trycloudflare.com/health\n\n# Transcribe an audio file (include the API key)\ncurl -X POST https://YOUR-URL.trycloudflare.com/v1/audio/transcriptions \\\n  -H \"Authorization: Bearer SK-COLAB-COMMUNITY\" \\\n  -F \"file=@recording.wav\" \\\n  -F \"response_format=verbose_json\"\n```\n\n### From MVP-Echo toolbar:\n\n1. Open Settings\n2. Set **Server URL** to your Colab public URL: `https://YOUR-URL.trycloudflare.com`\n3. Set **API Key** to: `SK-COLAB-COMMUNITY`\n4. Click **Test Connection** â€” it should succeed\n5. Start transcribing\n\n> **About the API key:** The toolbar requires an API key to complete its connection\n> test (`GET /v1/models`). The community key `SK-COLAB-COMMUNITY` satisfies this\n> check. Any non-empty key will work â€” this isn't real security, it's just\n> compatibility with the toolbar's connection flow.",
   "execution_count": null,
   "outputs": []
  }
 ]
}