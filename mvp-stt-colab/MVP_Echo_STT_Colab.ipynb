{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# MVP-Echo STT -- Try It on Colab\n\nRun **speech-to-text** powered by [NVIDIA Parakeet](https://huggingface.co/csukuangfj/sherpa-onnx-nemo-parakeet-tdt-0.6b-v2-int8) on a free Colab GPU.\n\n**No local GPU required.** This notebook gives you a temporary OpenAI-compatible\n`/v1/audio/transcriptions` endpoint backed by a T4 GPU.\n\n| | |\n|---|---|\n| **Model** | Parakeet TDT 0.6b v2 (INT8) -- English |\n| **Engine** | [sherpa-onnx](https://github.com/k2-fsa/sherpa-onnx) C++ with CUDA |\n| **VRAM** | ~426 MiB (fits easily on T4's 16GB) |\n| **API** | OpenAI Whisper-compatible |\n| **API Key** | `SK-COLAB-COMMUNITY` |\n\n### Why use this?\n\nGoogle Colab gives you **GPU-quality transcription accuracy** (~99%) without\nowning a GPU. The local CPU engine is fast but accuracy drops to ~70-80% --\nwords get garbled, sentences drift. If you don't have a GPU and you need\naccurate transcription, this is the way to try it.\n\n### Real-world benchmarks\n\nTested with a 67-word paragraph (4 sentences) read at natural speed:\n\n| Setup | Time | Accuracy | Cost |\n|---|---|---|---|\n| **Local GPU** (localhost) | 1.2 sec | ~99% | Your hardware |\n| **Local GPU + Cloudflare** | 1.9 sec | ~99% | Your hardware |\n| **Google Colab** (this notebook) | 7.3 sec | ~99% | Free |\n| **Local CPU** (no GPU) | 1.1 sec | ~70-80% | Your hardware |\n\nColab is slow because of network round-trips and shared infrastructure --\nnot the model. The same model on a budget local GPU (under 6 GB VRAM) runs\nin 1.2 seconds. But if you don't have a GPU, Colab gets you the accuracy\nfor free. As long as you keep utterances to 1-2 sentences, it's reasonable.\n\n**How to use:**\n1. Make sure the runtime is set to **GPU** (Runtime > Change runtime type > T4 GPU)\n2. Run all cells in order (Runtime > Run all)\n3. Copy the full endpoint URL printed at the end\n4. In MVP-Echo toolbar settings, paste the endpoint URL and use API key: **`SK-COLAB-COMMUNITY`**\n\n> **Connecting from MVP-Echo toolbar:** Paste the full endpoint URL (ends with\n> `/v1/audio/transcriptions`) and use `SK-COLAB-COMMUNITY` as the API key.\n> The toolbar requires a key to complete the connection test. Any non-empty key\n> will work.\n\n> This is a **community demo** for developers who want to try GPU-quality\n> transcription without owning a GPU. The session is temporary and will shut\n> down when Colab reclaims the runtime. No data is stored. No telemetry.\n> Audio is processed and immediately deleted."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Verify GPU"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "!nvidia-smi\n\nimport torch\nif torch.cuda.is_available():\n    gpu_name = torch.cuda.get_device_name(0)\n    vram = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n    print(f\"\\nGPU: {gpu_name} ({vram:.1f} GB VRAM)\")\n    print(\"CUDA is available -- good to go.\")\nelse:\n    print(\"\\nNo GPU detected!\")\n    print(\"Go to Runtime > Change runtime type > select T4 GPU\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Install sherpa-onnx + dependencies\n",
    "\n",
    "Downloads pre-built C++ binaries with CUDA 12 + cuDNN 9 support (~234 MB)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import os\n",
    "\n",
    "SHERPA_VERSION = \"v1.12.23\"\n",
    "SHERPA_URL = (\n",
    "    f\"https://github.com/k2-fsa/sherpa-onnx/releases/download/{SHERPA_VERSION}/\"\n",
    "    f\"sherpa-onnx-{SHERPA_VERSION}-cuda-12.x-cudnn-9.x-linux-x64-gpu.tar.bz2\"\n",
    ")\n",
    "\n",
    "if not os.path.exists(\"/opt/sherpa-onnx/bin/sherpa-onnx-offline-websocket-server\"):\n",
    "    print(\"Downloading sherpa-onnx pre-built binaries...\")\n",
    "    !curl -fSL -o /tmp/sherpa.tar.bz2 \"{SHERPA_URL}\"\n",
    "    !mkdir -p /opt/sherpa-onnx\n",
    "    !tar xjf /tmp/sherpa.tar.bz2 -C /opt/sherpa-onnx --strip-components=1\n",
    "    !rm /tmp/sherpa.tar.bz2\n",
    "    print(\"sherpa-onnx installed.\")\n",
    "else:\n",
    "    print(\"sherpa-onnx already installed.\")\n",
    "\n",
    "# Set PATH and LD_LIBRARY_PATH for this session\n",
    "os.environ[\"PATH\"] = f\"/opt/sherpa-onnx/bin:{os.environ['PATH']}\"\n",
    "os.environ[\"LD_LIBRARY_PATH\"] = f\"/opt/sherpa-onnx/lib:{os.environ.get('LD_LIBRARY_PATH', '')}\"\n",
    "\n",
    "# Verify\n",
    "!sherpa-onnx-offline-websocket-server --help 2>&1 | head -3\n",
    "print(\"\\nsherpa-onnx is ready.\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Install Python dependencies\n",
    "!pip install -q fastapi \"uvicorn[standard]\" python-multipart soundfile numpy websockets"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Download model\n",
    "\n",
    "Downloads the Parakeet TDT 0.6b v2 (INT8) model from HuggingFace (~300 MB).\n",
    "This is NVIDIA's English speech recognition model, quantized for fast inference.\n",
    "\n",
    "### HuggingFace Token (optional but recommended)\n",
    "\n",
    "The model is on a **public repo**, so it will download without a token. However,\n",
    "HuggingFace may rate-limit anonymous downloads. Adding a token avoids this.\n",
    "\n",
    "**How to get a HuggingFace token:**\n",
    "1. Create a free account at [huggingface.co](https://huggingface.co/join)\n",
    "2. Go to [Settings > Access Tokens](https://huggingface.co/settings/tokens)\n",
    "3. Click **\"New token\"** > name it anything (e.g. \"Colab\") > select **\"Read\"** access\n",
    "4. Copy the token (starts with `hf_...`)\n",
    "\n",
    "**Two ways to add it in Colab:**\n",
    "\n",
    "**Option 1 -- Colab Secrets (recommended, more secure):**\n",
    "1. Click the **key icon** in the left sidebar\n",
    "2. Click **\"Add new secret\"**\n",
    "3. Name: `HF_TOKEN` > Value: paste your token\n",
    "4. Toggle **\"Notebook access\"** ON\n",
    "\n",
    "**Option 2 -- Paste directly in the cell below** (less secure, visible in notebook)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#@title Download model { display-mode: \"form\" }\n",
    "\n",
    "#@markdown **HuggingFace token** (optional -- leave blank to download without auth):\n",
    "HF_TOKEN = \"\"  #@param {type:\"string\"}\n",
    "\n",
    "# Try Colab Secrets first, then fall back to the field above\n",
    "hf_token = None\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    hf_token = userdata.get(\"HF_TOKEN\")\n",
    "    if hf_token:\n",
    "        print(\"Using HuggingFace token from Colab Secrets.\")\n",
    "except (ImportError, Exception):\n",
    "    pass\n",
    "\n",
    "if not hf_token and HF_TOKEN.strip():\n",
    "    hf_token = HF_TOKEN.strip()\n",
    "    print(\"Using HuggingFace token from cell input.\")\n",
    "\n",
    "if not hf_token:\n",
    "    print(\"No HuggingFace token provided -- downloading anonymously.\")\n",
    "    print(\"(This works fine, but may be rate-limited on heavy usage days.)\\n\")\n",
    "\n",
    "MODEL_ID = \"parakeet-tdt-0.6b-v2-int8\"\n",
    "HF_REPO = f\"csukuangfj/sherpa-onnx-nemo-{MODEL_ID}\"\n",
    "MODEL_DIR = f\"/content/models/sherpa-onnx-nemo-{MODEL_ID}\"\n",
    "\n",
    "if os.path.exists(f\"{MODEL_DIR}/tokens.txt\"):\n",
    "    print(f\"Model already downloaded: {MODEL_ID}\")\n",
    "else:\n",
    "    print(f\"Downloading {MODEL_ID} from HuggingFace...\")\n",
    "    !pip install -q huggingface-hub\n",
    "    from huggingface_hub import snapshot_download\n",
    "    snapshot_download(\n",
    "        repo_id=HF_REPO,\n",
    "        local_dir=MODEL_DIR,\n",
    "        token=hf_token,\n",
    "    )\n",
    "    print(f\"Model downloaded to {MODEL_DIR}\")\n",
    "\n",
    "# Verify model files\n",
    "required = [\"encoder.int8.onnx\", \"decoder.int8.onnx\", \"joiner.int8.onnx\", \"tokens.txt\"]\n",
    "for f in required:\n",
    "    path = os.path.join(MODEL_DIR, f)\n",
    "    size_mb = os.path.getsize(path) / (1024 * 1024) if os.path.exists(path) else 0\n",
    "    status = f\"{size_mb:.1f} MB\" if os.path.exists(path) else \"MISSING\"\n",
    "    print(f\"  {f}: {status}\")\n",
    "print(f\"\\nModel ready: {MODEL_ID}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Start the STT server\n",
    "\n",
    "This starts:\n",
    "1. The sherpa-onnx C++ WebSocket server (loads model into GPU memory)\n",
    "2. A FastAPI HTTP server with the OpenAI-compatible `/v1/audio/transcriptions` endpoint\n",
    "\n",
    "The server includes a `/v1/models` endpoint so the MVP-Echo toolbar can\n",
    "complete its connection test. Use API key `SK-COLAB-COMMUNITY` (or any non-empty key)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Write the server script to disk\n\nSERVER_SCRIPT = r'''\nimport asyncio, json, os, signal, struct, subprocess, tempfile, time\nimport numpy as np\nimport soundfile as sf\nimport uvicorn\nimport websockets\nfrom contextlib import asynccontextmanager\nfrom fastapi import FastAPI, File, Form, Request, UploadFile\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom fastapi.responses import JSONResponse\n\nMODEL_DIR = os.environ.get(\"MODEL_DIR\")\nPROVIDER = os.environ.get(\"PROVIDER\", \"cuda\")\nNUM_THREADS = os.environ.get(\"NUM_THREADS\", \"4\")\nPORT = int(os.environ.get(\"PORT\", \"8000\"))\nWS_PORT = int(os.environ.get(\"WS_PORT\", \"7100\"))\n\nCOMMUNITY_API_KEY = \"SK-COLAB-COMMUNITY\"\n\n_process = None\n\nasync def start_sherpa():\n    global _process\n    cmd = [\n        \"sherpa-onnx-offline-websocket-server\",\n        f\"--port={WS_PORT}\", f\"--provider={PROVIDER}\",\n        f\"--encoder={MODEL_DIR}/encoder.int8.onnx\",\n        f\"--decoder={MODEL_DIR}/decoder.int8.onnx\",\n        f\"--joiner={MODEL_DIR}/joiner.int8.onnx\",\n        f\"--tokens={MODEL_DIR}/tokens.txt\",\n        f\"--num-threads={NUM_THREADS}\", \"--max-utterance-length=600\",\n    ]\n    print(f\"[server] Starting sherpa-onnx...\")\n    _process = await asyncio.create_subprocess_exec(\n        *cmd, stdout=asyncio.subprocess.PIPE, stderr=asyncio.subprocess.PIPE)\n    for _ in range(60):\n        if _process.returncode is not None:\n            stderr = \"\"\n            if _process.stderr:\n                try:\n                    raw = await asyncio.wait_for(_process.stderr.read(4096), 1.0)\n                    stderr = raw.decode(errors=\"replace\")\n                except asyncio.TimeoutError: pass\n            raise RuntimeError(f\"sherpa-onnx exited: {stderr}\")\n        try:\n            async with websockets.connect(f\"ws://localhost:{WS_PORT}\",\n                                          close_timeout=2, open_timeout=2) as ws:\n                await ws.send(\"Done\")\n            print(f\"[server] sherpa-onnx ready (PID={_process.pid}, provider={PROVIDER})\")\n            return\n        except: await asyncio.sleep(0.5)\n    raise RuntimeError(\"sherpa-onnx failed to start\")\n\ndef convert_to_wav(inp, out):\n    try:\n        r = subprocess.run([\"ffmpeg\",\"-y\",\"-loglevel\",\"error\",\"-i\",inp,\n            \"-ar\",\"16000\",\"-ac\",\"1\",\"-sample_fmt\",\"s16\",\"-f\",\"wav\",out],\n            capture_output=True, timeout=30)\n        return r.returncode == 0\n    except: return False\n\nasync def transcribe_ws(samples, sample_rate):\n    async with websockets.connect(f\"ws://localhost:{WS_PORT}\") as ws:\n        header = struct.pack(\"<ii\", sample_rate, samples.size * 4)\n        buf = header + samples.tobytes()\n        for start in range(0, len(buf), 10240):\n            await ws.send(buf[start:start+10240])\n        result = await ws.recv()\n        await ws.send(\"Done\")\n    try: return json.loads(result).get(\"text\", \"\").strip()\n    except: return result.strip() if isinstance(result, str) else result.decode().strip()\n\ndef _check_auth(request):\n    auth = request.headers.get(\"authorization\", \"\")\n    if not auth.startswith(\"Bearer \"): return False\n    return len(auth[7:].strip()) > 0\n\n@asynccontextmanager\nasync def lifespan(application):\n    await start_sherpa()\n    print(f\"[server] Ready on port {PORT}\")\n    print(f\"[server] Community API key: {COMMUNITY_API_KEY}\")\n    yield\n    if _process and _process.returncode is None:\n        _process.terminate()\n\napp = FastAPI(title=\"MVP-Echo STT\", lifespan=lifespan)\napp.add_middleware(CORSMiddleware, allow_origins=[\"*\"], allow_methods=[\"*\"], allow_headers=[\"*\"])\n\n@app.get(\"/health\")\nasync def health():\n    return {\"status\": \"ok\", \"model\": os.path.basename(MODEL_DIR), \"provider\": PROVIDER}\n\n@app.get(\"/v1/models\")\nasync def list_models(request: Request):\n    if not _check_auth(request):\n        return JSONResponse(status_code=401,\n            content={\"error\": \"Invalid or missing API key. Use: SK-COLAB-COMMUNITY\"})\n    model_id = os.path.basename(MODEL_DIR)\n    for prefix in (\"sherpa-onnx-nemo-\", \"sherpa-onnx-\"):\n        if model_id.startswith(prefix):\n            model_id = model_id[len(prefix):]; break\n    return JSONResponse({\"data\": [{\"id\": model_id, \"object\": \"model\",\n        \"owned_by\": \"colab\", \"label\": \"English\", \"group\": \"gpu\", \"active\": True}]})\n\n@app.post(\"/v1/audio/transcriptions\")\nasync def transcribe(\n    file: UploadFile = File(...), model: str = Form(default=\"\"),\n    language: str = Form(default=\"en\"), response_format: str = Form(default=\"verbose_json\"),\n    temperature: str = Form(default=\"0\"),\n):\n    t0 = time.time()\n    suffix = os.path.splitext(file.filename or \"audio.webm\")[1] or \".webm\"\n    with tempfile.NamedTemporaryFile(suffix=suffix, delete=False) as tmp:\n        tmp.write(await file.read()); tmp_path = tmp.name\n    wav_path = tmp_path + \".wav\"\n    try:\n        if not convert_to_wav(tmp_path, wav_path):\n            return JSONResponse(status_code=400, content={\"error\": \"Audio conversion failed\"})\n        samples, sr = sf.read(wav_path, dtype=\"float32\")\n        if len(samples.shape) > 1: samples = samples[:, 0]\n        dur = len(samples) / sr\n        text = await transcribe_ws(samples.astype(np.float32), sr)\n        elapsed = time.time() - t0\n        print(f\"[server] {dur:.1f}s -> {elapsed:.2f}s (RTF={elapsed/dur:.3f}): \\\"{text[:80]}\\\"\")\n        if response_format == \"verbose_json\":\n            return JSONResponse({\"text\": text, \"language\": language,\n                \"duration\": round(dur, 2),\n                \"segments\": [{\"id\":0,\"start\":0.0,\"end\":round(dur,2),\"text\":text}]})\n        return JSONResponse({\"text\": text})\n    except Exception as e:\n        return JSONResponse(status_code=500, content={\"error\": str(e)})\n    finally:\n        for p in [tmp_path, wav_path]:\n            try: os.unlink(p)\n            except: pass\n\nif __name__ == \"__main__\":\n    uvicorn.run(app, host=\"0.0.0.0\", port=PORT)\n'''.strip()\n\nwith open('/content/server.py', 'w') as f:\n    f.write(SERVER_SCRIPT)\nprint('Server script written to /content/server.py')",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import subprocess as _sp\n",
    "import time as _time\n",
    "import os as _os\n",
    "\n",
    "# Set environment for the server process\n",
    "server_env = _os.environ.copy()\n",
    "server_env[\"MODEL_DIR\"] = MODEL_DIR\n",
    "server_env[\"PROVIDER\"] = \"cuda\"\n",
    "server_env[\"PORT\"] = \"8000\"\n",
    "\n",
    "# Start the server in the background\n",
    "server_proc = _sp.Popen(\n",
    "    [\"python3\", \"/content/server.py\"],\n",
    "    env=server_env,\n",
    "    stdout=_sp.PIPE,\n",
    "    stderr=_sp.STDOUT,\n",
    ")\n",
    "print(f\"Server starting (PID={server_proc.pid})...\")\n",
    "\n",
    "# Wait for the server to be ready\n",
    "import urllib.request\n",
    "for i in range(120):\n",
    "    try:\n",
    "        resp = urllib.request.urlopen(\"http://localhost:8000/health\", timeout=2)\n",
    "        data = resp.read().decode()\n",
    "        print(f\"\\nServer is up! Health: {data}\")\n",
    "        break\n",
    "    except Exception:\n",
    "        if server_proc.poll() is not None:\n",
    "            output = server_proc.stdout.read().decode()\n",
    "            print(f\"Server died! Output:\\n{output}\")\n",
    "            break\n",
    "        print(\".\", end=\"\", flush=True)\n",
    "        _time.sleep(1)\n",
    "else:\n",
    "    print(\"\\nServer failed to start within 120 seconds.\")\n",
    "    output = server_proc.stdout.read(4096).decode() if server_proc.stdout else \"\"\n",
    "    print(f\"Output: {output}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Create public URL\n",
    "\n",
    "Choose **one** of the options below to expose the server with a public URL.\n",
    "\n",
    "**Option A** (cloudflared) -- no signup needed, works immediately.\n",
    "\n",
    "**Option B** (ngrok) -- requires a free [ngrok.com](https://ngrok.com) account."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "#@title Option A: cloudflared tunnel (no signup required)\n\nimport subprocess, time, re\n\n# Install cloudflared\n!wget -q https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64 -O /usr/local/bin/cloudflared\n!chmod +x /usr/local/bin/cloudflared\n\n# Start tunnel\ntunnel_proc = subprocess.Popen(\n    [\"cloudflared\", \"tunnel\", \"--url\", \"http://localhost:8000\", \"--no-autoupdate\"],\n    stdout=subprocess.PIPE,\n    stderr=subprocess.PIPE,\n)\n\n# Wait for the URL to appear in stderr\npublic_url = None\ndeadline = time.time() + 30\nbuffer = b\"\"\n\nimport select\nwhile time.time() < deadline and public_url is None:\n    ready, _, _ = select.select([tunnel_proc.stderr], [], [], 1.0)\n    if ready:\n        chunk = tunnel_proc.stderr.read1(4096) if hasattr(tunnel_proc.stderr, 'read1') else tunnel_proc.stderr.read(4096)\n        if chunk:\n            buffer += chunk\n            text = buffer.decode(errors=\"replace\")\n            match = re.search(r'(https://[a-z0-9-]+\\.trycloudflare\\.com)', text)\n            if match:\n                public_url = match.group(1)\n\nif public_url:\n    endpoint_url = f\"{public_url}/v1/audio/transcriptions\"\n    print(\"=\" * 70)\n    print(\"Your server is ready! Copy these into MVP-Echo toolbar:\\n\")\n    print(f\"  Endpoint URL: {endpoint_url}\")\n    print(f\"  API Key:      SK-COLAB-COMMUNITY\")\n    print()\n    print(\"=\" * 70)\n    print(f\"\\nHealth check:  curl {public_url}/health\")\n    print(f\"\\nThis URL is temporary and will stop when the Colab session ends.\")\nelse:\n    print(\"Failed to get tunnel URL. Check the output above for errors.\")\n    print(\"Stderr:\", buffer.decode(errors=\"replace\")[:500])",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "#@title Option B: ngrok tunnel (requires free account)\n\n# 1. Sign up at https://ngrok.com (free)\n# 2. Get your auth token from https://dashboard.ngrok.com/get-started/your-authtoken\n# 3. Paste it below:\n\nNGROK_AUTH_TOKEN = \"\"  #@param {type:\"string\"}\n\nif not NGROK_AUTH_TOKEN:\n    print(\"Paste your ngrok auth token above and re-run this cell.\")\n    print(\"Get one free at: https://dashboard.ngrok.com/get-started/your-authtoken\")\nelse:\n    !pip install -q pyngrok\n    from pyngrok import ngrok\n\n    ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n    tunnel = ngrok.connect(8000, \"http\")\n    public_url = tunnel.public_url\n    endpoint_url = f\"{public_url}/v1/audio/transcriptions\"\n\n    print(\"=\" * 70)\n    print(\"Your server is ready! Copy these into MVP-Echo toolbar:\\n\")\n    print(f\"  Endpoint URL: {endpoint_url}\")\n    print(f\"  API Key:      SK-COLAB-COMMUNITY\")\n    print()\n    print(\"=\" * 70)\n    print(f\"\\nHealth check:  curl {public_url}/health\")\n    print(f\"\\nThis URL is temporary and will stop when the Colab session ends.\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Test it\n",
    "\n",
    "### From this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Quick health check\n",
    "!curl -s http://localhost:8000/health | python3 -m json.tool"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Test transcription pipeline with a silence clip\n",
    "\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "\n",
    "sr = 16000\n",
    "duration = 2.0\n",
    "samples = np.zeros(int(sr * duration), dtype=np.float32)\n",
    "sf.write(\"/tmp/test_silence.wav\", samples, sr)\n",
    "\n",
    "print(\"Transcribing test audio (2s silence)...\")\n",
    "!curl -s -X POST http://localhost:8000/v1/audio/transcriptions \\\n",
    "  -F \"file=@/tmp/test_silence.wav\" \\\n",
    "  -F \"response_format=verbose_json\" | python3 -m json.tool\n",
    "\n",
    "print(\"\\nTo test with real audio, upload a file and run:\")\n",
    "print('  !curl -X POST http://localhost:8000/v1/audio/transcriptions -F \"file=@/path/to/audio.wav\"')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### From your machine (using the public URL):\n\n```bash\n# Health check\ncurl https://YOUR-URL.trycloudflare.com/health\n\n# Transcribe an audio file (include the API key)\ncurl -X POST https://YOUR-URL.trycloudflare.com/v1/audio/transcriptions \\\n  -H \"Authorization: Bearer SK-COLAB-COMMUNITY\" \\\n  -F \"file=@recording.wav\" \\\n  -F \"response_format=verbose_json\"\n```\n\n### From MVP-Echo toolbar:\n\n1. Open **Settings**\n2. Set **Endpoint URL** to the full URL printed above (ends with `/v1/audio/transcriptions`)\n3. Set **API Key** to: `SK-COLAB-COMMUNITY`\n4. Click **Test Connection** -- it should succeed\n5. Start transcribing\n\n> **About the API key:** The toolbar requires an API key to complete its connection\n> test (`GET /v1/models`). The community key `SK-COLAB-COMMUNITY` satisfies this\n> check. Any non-empty key will work."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Keep alive\n",
    "\n",
    "Colab will disconnect after ~90 minutes of inactivity (free tier).\n",
    "Run this cell to keep the session alive while you are using it.\n",
    "**Stop it manually when you are done** (click the stop button on this cell)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import time\n",
    "\n",
    "print(\"Keeping session alive. Stop this cell when you are done.\")\n",
    "while True:\n",
    "    time.sleep(60)\n",
    "    print(\".\", end=\"\", flush=True)"
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}